{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0a6b4-13c3-4729-8c9b-65a60be8f448",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## DATASET EXTRACTION AND WINDOWING   #########################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "# === List of CSV paths ===\n",
    "csv_paths = [\n",
    "    \"misreport_long_data-Copy/augmented_dataset_with_behavioral.csv\",\n",
    "    \"misreport_long_data2/augmented_dataset_with_behavioral.csv\",\n",
    "    \"misreport_long_data3/augmented_dataset_with_behavioral.csv\",\n",
    "    \"misreport_long_data4/augmented_dataset_with_behavioral.csv\"\n",
    "]\n",
    "\n",
    "# === Generate sliding windows per switch ===\n",
    "def generate_windowed_rows(df, switch_ids, window_size=10, stride=5):\n",
    "    rows_by_switch = defaultdict(list)\n",
    "    for sw in switch_ids:\n",
    "        feature_cols = [\n",
    "            f\"Load_{sw}\",\n",
    "            f\"Load_{sw}_delta\",\n",
    "            f\"Load_{sw}_rollmean\",\n",
    "            f\"Load_{sw}_percentile\",\n",
    "            f\"Load_{sw}_rolling_percentile\",\n",
    "            f\"Load_{sw}_std_recent\",\n",
    "            f\"Load_{sw}_load_ratio\",\n",
    "            f\"Load_{sw}_delta_mean\",\n",
    "            f\"Load_{sw}_mad\",\n",
    "            f\"Load_{sw}_unique_count\",\n",
    "            f\"Load_{sw}_autocorr\",\n",
    "            f\"Load_{sw}_skew\",\n",
    "            f\"Load_{sw}_kurtosis\",\n",
    "            f\"Load_{sw}_zscore\"\n",
    "        ]\n",
    "        label_col = f\"Label_{sw}\"\n",
    "\n",
    "        if not all(col in df.columns for col in feature_cols + [label_col]):\n",
    "            print(f\"âš ï¸ Skipping switch {sw} â€” missing columns\")\n",
    "            continue\n",
    "\n",
    "        for i in range(0, len(df) - window_size + 1, stride):\n",
    "            window_feats = df[feature_cols].iloc[i:i+window_size].values.astype(np.float32)\n",
    "            window_label = df[label_col].iloc[i:i+window_size].values\n",
    "            label = int(np.any(window_label > 0.5))  # Binary label\n",
    "            rows_by_switch[sw].append({\n",
    "                \"switch\": sw,\n",
    "                \"features\": window_feats,\n",
    "                \"label\": label\n",
    "            })\n",
    "    return rows_by_switch\n",
    "\n",
    "# === Label stats print ===\n",
    "def print_label_distribution(windowed_rows, name=\"\"):\n",
    "    labels = [row[\"label\"] for row in windowed_rows]\n",
    "    counts = Counter(labels)\n",
    "    print(f\"\\nðŸ“¦ Label Distribution for {name}:\")\n",
    "    print(f\"FAKE = {counts.get(1, 0)}, REAL = {counts.get(0, 0)}\")\n",
    "\n",
    "# === Accumulate and shuffle windows ===\n",
    "combined_rows_by_switch = defaultdict(list)\n",
    "\n",
    "for path in tqdm(csv_paths, desc=\"Processing CSVs\"):\n",
    "    df = pd.read_csv(path)\n",
    "    print(f\"\\nâœ… Loaded dataset: {path} â€” shape: {df.shape}\")\n",
    "    \n",
    "    # Identify switch IDs\n",
    "    load_cols = [col for col in df.columns if re.fullmatch(r\"Load_S\\d+\", col)]\n",
    "    switch_ids = [col.split(\"Load_\")[1] for col in load_cols]\n",
    "    print(f\"âœ… Found switch IDs: {switch_ids}\")\n",
    "    \n",
    "    # Normalize features (excluding labels)\n",
    "    label_cols = [col for col in df.columns if re.fullmatch(r\"Label_S\\d+\", col)]\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    feature_cols_to_scale = list(set(numeric_cols) - set(label_cols))\n",
    "    scaler = StandardScaler()\n",
    "    df[feature_cols_to_scale] = scaler.fit_transform(df[feature_cols_to_scale])\n",
    "    print(\"âœ… Standardized all numeric features\")\n",
    "\n",
    "    # Generate windowed data\n",
    "    file_windows = generate_windowed_rows(df, switch_ids, window_size=10, stride=5)\n",
    "    for sw, windows in file_windows.items():\n",
    "        combined_rows_by_switch[sw].extend(windows)\n",
    "\n",
    "# === Shuffle windows per switch ===\n",
    "for sw in combined_rows_by_switch:\n",
    "    random.shuffle(combined_rows_by_switch[sw])\n",
    "print(\"âœ… Shuffled windows within each switch\")\n",
    "\n",
    "# === Combine into single list ===\n",
    "windowed_rows_10 = []\n",
    "for sw_windows in combined_rows_by_switch.values():\n",
    "    windowed_rows_10.extend(sw_windows)\n",
    "print(\"âœ… Combined all switches' windows into one list\")\n",
    "\n",
    "# === Final global shuffle ===\n",
    "random.shuffle(windowed_rows_10)\n",
    "print(\"âœ… Globally shuffled the final windowed dataset\")\n",
    "\n",
    "print_label_distribution(windowed_rows_10, \"Final Window10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e7fd75-faeb-4d5f-9e41-b01ccdb84d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Hybrid Anomaly Detection with SHAP + MLP/LightGBM  ###################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "!pip install shap lightgbm\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "################## 1. Prepare Data ##################\n",
    "X = np.array([row[\"features\"] for row in windowed_rows_10])\n",
    "y = np.array([row[\"label\"] for row in windowed_rows_10])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "X_train_real = X_train[y_train == 0]\n",
    "\n",
    "\n",
    "################## Classification report printing 5 decimal places  ############################\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def print_full_classification_report(y_true, y_pred, digits=5):\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    header = f\"{'Label':<15} {'Precision':>{digits+7}} {'Recall':>{digits+7}} {'F1-Score':>{digits+7}} {'Support':>10}\"\n",
    "    print(header)\n",
    "    print(\"=\" * len(header))\n",
    "\n",
    "    for label, metrics in report.items():\n",
    "        if isinstance(metrics, dict):\n",
    "            precision = f\"{metrics['precision']:.{digits}f}\"\n",
    "            recall = f\"{metrics['recall']:.{digits}f}\"\n",
    "            f1 = f\"{metrics['f1-score']:.{digits}f}\"\n",
    "            support = f\"{int(metrics['support']):>10}\"\n",
    "            print(f\"{label:<15} {precision:>{digits+7}} {recall:>{digits+7}} {f1:>{digits+7}} {support}\")\n",
    "    \n",
    "    # Accuracy is a float, not a dict\n",
    "    acc = report['accuracy']\n",
    "    acc_str = f\"{acc:.{digits}f}\"\n",
    "    print(\"=\" * len(header))\n",
    "    print(f\"{'Accuracy':<15} {'':>{digits+7}} {'':>{digits+7}} {acc_str:>{digits+7}}\")\n",
    "\n",
    "####################################################################################################\n",
    "\n",
    "################## 2. Transformer Autoencoder ##################\n",
    "class TransformerAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, seq_len, d_model=64, nhead=4, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, seq_len, d_model) * 0.02)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dropout=0.1, batch_first=True, norm_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.decoder = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, input_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_proj = self.input_proj(x) + self.pos_embedding\n",
    "        encoded = self.encoder(x_proj)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded, encoded\n",
    "\n",
    "def train_autoencoder(model, X_train, device, epochs=100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(epochs):\n",
    "        batch_size = 256\n",
    "        perm = np.random.permutation(len(X_train))\n",
    "        losses = []\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            idx = perm[i:i+batch_size]\n",
    "            xb = torch.tensor(X_train[idx], dtype=torch.float32).to(device)\n",
    "            decoded, _ = model(xb)\n",
    "            loss = criterion(decoded, xb)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "        print(f\"Epoch {epoch+1}, Loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "def score_model(model, X, device, batch_size=512):\n",
    "    model.eval()\n",
    "    recon_errors, latents = [], []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X), batch_size):\n",
    "            xb = torch.tensor(X[i:i+batch_size], dtype=torch.float32).to(device)\n",
    "            decoded, encoded = model(xb)\n",
    "            recon_error = ((decoded - xb) ** 2).mean(dim=(1, 2)).cpu().numpy()\n",
    "            latent = encoded.mean(dim=1).cpu().numpy()\n",
    "            recon_errors.extend(recon_error)\n",
    "            latents.extend(latent)\n",
    "    return np.array(recon_errors), np.array(latents)\n",
    "\n",
    "################## 3. Statistical Baseline ##################\n",
    "class StatisticalAutoencoder:\n",
    "    def fit(self, X):\n",
    "        self.mean = X.mean(axis=(0,1))\n",
    "        self.std = X.std(axis=(0,1))\n",
    "    def score(self, X):\n",
    "        return (((X - self.mean) / (self.std + 1e-6))**2).mean(axis=(1,2))\n",
    "\n",
    "################## 4. Train Autoencoder ##################\n",
    "seq_len, input_dim = X.shape[1], X.shape[2]\n",
    "model = TransformerAutoencoder(input_dim, seq_len).to(device)\n",
    "train_autoencoder(model, X_train_real, device)\n",
    "\n",
    "################## 5. Score ##################\n",
    "recon_val, latent_val = score_model(model, X_val, device)\n",
    "recon_test, latent_test = score_model(model, X_test, device)\n",
    "latent_train = score_model(model, X_train_real, device)[1]\n",
    "\n",
    "stat_model = StatisticalAutoencoder()\n",
    "stat_model.fit(X_train_real)\n",
    "stat_val = stat_model.score(X_val)\n",
    "stat_test = stat_model.score(X_test)\n",
    "\n",
    "emp_cov = EmpiricalCovariance().fit(latent_train)\n",
    "mahal_val = emp_cov.mahalanobis(latent_val)\n",
    "mahal_test = emp_cov.mahalanobis(latent_test)\n",
    "\n",
    "################## 6. Fusion Features ##################\n",
    "val_feats = np.stack([recon_val, stat_val, mahal_val], axis=1)\n",
    "test_feats = np.stack([recon_test, stat_test, mahal_test], axis=1)\n",
    "\n",
    "scaler_main = StandardScaler()\n",
    "val_feats_scaled = scaler_main.fit_transform(val_feats)\n",
    "test_feats_scaled = scaler_main.transform(test_feats)\n",
    "\n",
    "################## 7. Shallow MLP Fusion ##################\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(16,), max_iter=500, random_state=42)\n",
    "mlp_model.fit(val_feats_scaled, y_val)\n",
    "mlp_preds = mlp_model.predict(test_feats_scaled)\n",
    "mlp_probs = mlp_model.predict_proba(test_feats_scaled)[:, 1]\n",
    "\n",
    "print(\"\\nðŸ“Š MLP Final Test Report:\")\n",
    "print(classification_report(y_test, mlp_preds, target_names=[\"REAL\", \"FAKE\"]))\n",
    "print(\"MLP Test ROC AUC:\", roc_auc_score(y_test, mlp_probs))\n",
    "print_full_classification_report(y_test, mlp_preds)\n",
    "\n",
    "\n",
    "################## 8. LightGBM + Calibration ##################\n",
    "lgb_model = lgb.LGBMClassifier(n_estimators=100, max_depth=2)\n",
    "calibrated_lgb = CalibratedClassifierCV(estimator=lgb_model, method='sigmoid', cv=5)\n",
    "calibrated_lgb.fit(val_feats_scaled, y_val)\n",
    "lgb_preds = calibrated_lgb.predict(test_feats_scaled)\n",
    "lgb_probs = calibrated_lgb.predict_proba(test_feats_scaled)[:, 1]\n",
    "\n",
    "print(\"\\nðŸ“Š Calibrated LightGBM Final Test Report:\")\n",
    "print(classification_report(y_test, lgb_preds, target_names=[\"REAL\", \"FAKE\"]))\n",
    "print(\"LightGBM Test ROC AUC:\", roc_auc_score(y_test, lgb_probs))\n",
    "print_full_classification_report(y_test, lgb_preds)\n",
    "\n",
    "\n",
    "################## 9. SHAP Analysis (corrected) ##################\n",
    "# Use the fitted base estimator from the calibrated model\n",
    "fitted_lgb = calibrated_lgb.calibrated_classifiers_[0].estimator  # Use .estimators_[0][0] if this fails\n",
    "\n",
    "explainer = shap.TreeExplainer(fitted_lgb)\n",
    "shap_values = explainer.shap_values(test_feats_scaled)\n",
    "shap.summary_plot(shap_values, test_feats_scaled, feature_names=[\"recon\", \"stat\", \"mahal\"], show=False)\n",
    "#plt.savefig(\"plots/hybrid-shap_summary_plot.png\", dpi=600, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "################## 10. t-SNE Latent Visualization ##################\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "latent_2d = tsne.fit_transform(latent_test)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.title(\"Latent space (TSNE projection)\")\n",
    "plt.scatter(latent_2d[y_test==0, 0], latent_2d[y_test==0, 1], c='blue', label=\"REAL\", alpha=0.5)\n",
    "plt.scatter(latent_2d[y_test==1, 0], latent_2d[y_test==1, 1], c='red', label=\"FAKE\", alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel(\"TSNE-1\")\n",
    "plt.ylabel(\"TSNE-2\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"plots/hybrid-tsne_latent_space.png\", dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "############################ CLEAN CODE  #################################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
